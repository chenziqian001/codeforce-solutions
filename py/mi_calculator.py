import re
import math
from collections import Counter, defaultdict
from pprint import pprint
import os

# ====================================================================
# é€šç”¨å·¥å…·å‡½æ•° (ä¸åŸä»£ç ä¿æŒä¸€è‡´)
# ====================================================================

def preprocess_char_text(text):
    """
    å•ä¸ªæ–‡æœ¬ä»»åŠ¡ï¼šé¢„å¤„ç†ï¼Œç”¨äºå­—ç¬¦çº§åˆ†æã€‚
    """
    text = text.lower()
    text = re.sub(r'[^a-z0-9]', '', text)
    return text

def tokenize_text_for_word_analysis(text):
    """
    ä¸¤ä¸ªæ–‡æœ¬ä»»åŠ¡ï¼šé¢„å¤„ç†å¹¶ç¬¦å·åŒ–ï¼ŒåŸºäºè¯è¯­çº§åˆ†æã€‚
    """
    text = text.lower()
    text = re.sub(r'[^a-z0-9\s]', ' ', text)
    tokens = re.sub(r'\s+', ' ', text).strip().split(' ')
    return [t for t in tokens if t]

# ====================================================================
# ä»»åŠ¡ 1ï¼šå•ä¸ªæ–‡æœ¬çš„ç‚¹äº’ä¿¡æ¯ä¸å¹³å‡äº’ä¿¡æ¯è®¡ç®— (ä¸åŸä»£ç ä¿æŒä¸€è‡´)
# ====================================================================

def calculate_single_text_mi(file_path):
    """
    è®¡ç®—å•ä¸ªæ–‡æœ¬çš„ç›¸é‚»ç¬¦å·ï¼ˆå­—ç¬¦ï¼‰ç‚¹äº’ä¿¡æ¯å’Œå¹³å‡äº’ä¿¡æ¯ã€‚
    """
    print("## ğŸ¤– ä»»åŠ¡ 1ï¼šå•ä¸ªæ–‡æœ¬çš„ç‚¹äº’ä¿¡æ¯ä¸å¹³å‡äº’ä¿¡æ¯è®¡ç®—")
    print("--------------------------------------------------")
    
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            raw_text = f.read()
    except FileNotFoundError:
        print(f"é”™è¯¯ï¼šæ–‡ä»¶æœªæ‰¾åˆ° '{file_path}'")
        return

    # 1-2. æ•°æ®è¯»å–ä¸é¢„å¤„ç†
    processed_text = preprocess_char_text(raw_text)
    
    # 3. ç¬¦å·å®šä¹‰ï¼šç›¸é‚»å­—ç¬¦
    symbols = list(processed_text)
    N = len(symbols) # æ–‡æœ¬æ€»ç¬¦å·æ•°
    N_pairs = max(0, N - 1) # ç›¸é‚»ç¬¦å·å¯¹æ€»æ•°

    if N_pairs < 2:
        print("æ–‡æœ¬ç¬¦å·ä¸è¶³ï¼Œæ— æ³•è®¡ç®—ç›¸é‚»ç¬¦å·äº’ä¿¡æ¯ã€‚")
        return

    print(f"* æ–‡ä»¶è·¯å¾„: {file_path}")
    print(f"* é¢„å¤„ç†åæ–‡æœ¬ (ä»…æ˜¾ç¤ºå‰50å­—ç¬¦): {processed_text[:50]}...")
    print(f"* ç¬¦å·æ€»æ•° (N): {N}, ç›¸é‚»ç¬¦å·å¯¹æ€»æ•°: {N_pairs}")
    print("-" * 30)
    
    # 4. ç»Ÿè®¡è®¡ç®—
    count_x = Counter(symbols)
    count_xy = Counter()
    for i in range(N_pairs):
        pair = (symbols[i], symbols[i+1])
        count_xy[pair] += 1
        
    prob_x = {x: count / N for x, count in count_x.items()}
    prob_xy = {pair: count / N_pairs for pair, count in count_xy.items()}
    
    # 5. äº’ä¿¡æ¯è®¡ç®—
    pmi_list = []
    avg_mi = 0.0
    
    for (x, y), p_xy in prob_xy.items():
        p_x = prob_x[x]
        p_y = prob_x.get(y, 1e-10) 
        
        if p_xy == 0 or p_x == 0 or p_y == 0:
            pmi_xy = 0.0
        else:
            try:
                pmi_xy = math.log2(p_xy / (p_x * p_y))
            except ValueError:
                 pmi_xy = 0.0
        
        pmi_list.append({
            'pair': (x, y),
            'PMI': pmi_xy,
            'P(x,y)': p_xy,
            'P(x)': p_x,
            'P(y)': p_y
        })
        
        avg_mi += p_xy * pmi_xy
        
    # 6. ç»“æœè¾“å‡º
    print("### 1. å…³é”®ä¸­é—´æ•°æ® (ä»»åŠ¡ 1)")
    print("--- ç¬¦å·é¢‘ç‡ P(x) ---")
    pprint({k: f"{v:.4f}" for k, v in dict(list(prob_x.items())[:10]).items()}) 
    print("\n--- ç›¸é‚»ç¬¦å·å¯¹æ¦‚ç‡ P(x,y) ---")
    pprint({k: f"{v:.6f}" for k, v in dict(list(prob_xy.items())[:10]).items()}) 
    print("-" * 30)

    print("### 2. è®¡ç®—ç»“æœè¾“å‡º (ä»»åŠ¡ 1)")
    sorted_pmi = sorted(pmi_list, key=lambda x: x['PMI'], reverse=True)
    print("\n--- ç‚¹äº’ä¿¡æ¯ (PMI) Top 5 ---")
    for item in sorted_pmi[:5]:
        print(f"Pair: {item['pair']} | PMI: {item['PMI']:.4f} | P(x,y): {item['P(x,y)']:.6f}")
        
    print(f"\n--- æ–‡æœ¬å¹³å‡äº’ä¿¡æ¯ (Avg_MI) ---")
    print(f"Avg_MI: {avg_mi:.6f} bits/symbol")
    print("-" * 30)

    print("### 3. æ„ä¹‰ä¸ç”¨é€”æ€»ç»“ (ä»»åŠ¡ 1)")
    print("> **ç‚¹äº’ä¿¡æ¯ (PMI)**")
    print("  - **æ„ä¹‰:** è¡¡é‡ä¸¤ä¸ªå…·ä½“ç›¸é‚»ç¬¦å·ï¼ˆäº‹ä»¶ï¼‰çš„å…³è”å¼ºåº¦ã€‚")
    print("  - **ç”¨é€”:** è¯­è¨€å­¦åˆ†æï¼Œè‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„ç‰¹å¾å·¥ç¨‹ï¼Œåºåˆ—æ•°æ®åˆ†æã€‚")
    print("> **å¹³å‡äº’ä¿¡æ¯ (Avg_MI)**")
    print("  - **æ„ä¹‰:** åæ˜ æ–‡æœ¬ä¸­ç›¸é‚»ç¬¦å·çš„æ•´ä½“å…³è”ç¨‹åº¦ã€‚å€¼è¶Šé«˜ï¼Œæ–‡æœ¬å†—ä½™åº¦è¶Šé«˜ã€‚")
    print("  - **ç”¨é€”:** æ–‡æœ¬å‹ç¼©ï¼Œè¯„ä¼°è¯­è¨€æ¨¡å‹çš„ç»“æ„å¤æ‚åº¦ã€‚")
    print("=" * 70)


# ====================================================================
# ä»»åŠ¡ 2ï¼šä¸¤ä¸ªæ–‡æœ¬é—´çš„å¹³å‡äº’ä¿¡æ¯è®¡ç®— (ä¸åŸä»£ç ä¿æŒä¸€è‡´)
# ====================================================================

def calculate_two_text_avg_mi(file_path_a, file_path_b):
    """
    è®¡ç®—ä¸¤ä¸ªæ–‡æœ¬æ–‡ä»¶ä¹‹é—´çš„å¹³å‡äº’ä¿¡æ¯ï¼ŒåŸºäºè¯è¯­ç¬¦å·åŒ–ã€‚
    """
    print("\n## ğŸš€ ä»»åŠ¡ 2ï¼šä¸¤ä¸ªæ–‡æœ¬é—´çš„å¹³å‡äº’ä¿¡æ¯è®¡ç®—")
    print("--------------------------------------------------")
    
    try:
        with open(file_path_a, 'r', encoding='utf-8') as f:
            raw_text_a = f.read()
        with open(file_path_b, 'r', encoding='utf-8') as f:
            raw_text_b = f.read()
    except FileNotFoundError as e:
        print(f"é”™è¯¯ï¼šæ–‡ä»¶æœªæ‰¾åˆ° {e.filename}")
        return

    # 1-2. æ•°æ®è¯»å–ä¸é¢„å¤„ç†
    tokens_a = tokenize_text_for_word_analysis(raw_text_a)
    tokens_b = tokenize_text_for_word_analysis(raw_text_b)
    
    if not tokens_a or not tokens_b:
        print("è‡³å°‘ä¸€ä¸ªæ–‡æœ¬å†…å®¹ä¸ºç©ºï¼Œæ— æ³•è®¡ç®—ã€‚")
        return

    print(f"* æ–‡æ¡£ A è¯è¯­æ€»æ•° (N_A): {len(tokens_a)}")
    print(f"* æ–‡æ¡£ B è¯è¯­æ€»æ•° (N_B): {len(tokens_b)}")
    print("-" * 30)

    # 4. å»ºç«‹æ¦‚ç‡æ¨¡å‹
    vocab_a = set(tokens_a)
    vocab_b = set(tokens_b)
    full_vocab = vocab_a | vocab_b
    count_a = Counter(tokens_a)
    count_b = Counter(tokens_b)
    N_A = len(tokens_a)
    N_B = len(tokens_b)
    V_size = len(full_vocab)
    
    # 4.3. å…±ç°é¢‘ç‡ count_AB(x, y) - åŸºäºå¯¹é½çš„æ¨¡å‹
    count_ab = defaultdict(int)
    for word in full_vocab:
        if word in vocab_a and word in vocab_b:
            count_ab[(word, word)] = count_a[word] * count_b[word] 
        
    # 4.4. æ¦‚ç‡è®¡ç®—
    prob_a = {x: count / N_A for x, count in count_a.items()}
    prob_b = {y: count / N_B for y, count in count_b.items()}
    
    # P(x, y) = count_AB(x, y) / (V_size)^2
    V_squared = V_size * V_size
    prob_xy = {pair: count / V_squared for pair, count in count_ab.items()}
    
    # 5. å¹³å‡äº’ä¿¡æ¯è®¡ç®—
    avg_mi_ab = 0.0
    
    for (x, y), p_xy in prob_xy.items():
        p_a_x = prob_a.get(x, 1e-10)
        p_b_y = prob_b.get(y, 1e-10)
        
        if p_xy == 0:
            pmi_xy = 0.0
        else:
            try:
                pmi_xy = math.log2(p_xy / (p_a_x * p_b_y))
            except ValueError:
                pmi_xy = 0.0
        
        avg_mi_ab += p_xy * pmi_xy
        
    # 6. ç»“æœè¾“å‡º
    print("### 1. å…³é”®ä¸­é—´æ•°æ® (ä»»åŠ¡ 2)")
    print(f"--- ç»Ÿä¸€è¯æ±‡è¡¨å¤§å° (V_size): {V_size}")
    print("--- Doc A æ¦‚ç‡ P(A)(x) Top 5 ---")
    pprint({k: f"{v:.4f}" for k, v in dict(list(prob_a.items())[:5]).items()}) 
    print("\n--- å…±ç°æ¦‚ç‡ P(x,y) (ä»…æ˜¾ç¤º x=y çš„é¡¹) Top 5 ---")
    pprint({k: f"{v:.6f}" for k, v in dict(list(prob_xy.items())[:5]).items()}) 
    print("-" * 30)

    print("### 2. è®¡ç®—ç»“æœè¾“å‡º (ä»»åŠ¡ 2)")
    print(f"\n--- ä¸¤ä¸ªæ–‡æ¡£é—´çš„å¹³å‡äº’ä¿¡æ¯ (Avg_MI) ---")
    print(f"Avg_MI(A, B): {avg_mi_ab:.6f} bits")
    print("-" * 30)

    print("### 3. æ„ä¹‰ä¸ç”¨é€”æ€»ç»“ (ä»»åŠ¡ 2)")
    print("> **ä¸¤ä¸ªæ–‡æœ¬é—´çš„å¹³å‡äº’ä¿¡æ¯ (Avg_MI(A, B))**")
    print("  - **æ„ä¹‰:** è¡¡é‡æ–‡æ¡£Aå’Œæ–‡æ¡£Bç¬¦å·åˆ†å¸ƒçš„ç»Ÿè®¡ä¾èµ–æ€§ã€‚")
    print("  - **ç”¨é€”ï¼ˆåˆ¤å®šæ–‡æ¡£ç›¸å…³æ€§ï¼‰:** Avg_MI å€¼è¶Šå¤§ï¼Œè¡¨ç¤ºä¸¤ä¸ªæ–‡æ¡£çš„ç»Ÿè®¡ç‰¹æ€§è¶Šç›¸ä¼¼ï¼Œç›¸å…³æ€§è¶Šé«˜ã€‚")
    print("=" * 70)


# ====================================================================
# è¿è¡Œç¤ºä¾‹ï¼ˆæ›´æ–°è·¯å¾„ï¼‰
# ====================================================================

# è¯·å°† BASE_PATH è®¾ç½®ä¸ºæ‚¨æä¾›çš„æ–‡ä»¶å¤¹è·¯å¾„
BASE_PATH = r'C:\Users\62685\Desktop\tester'

# å®Œæ•´çš„æµ‹è¯•æ–‡ä»¶è·¯å¾„
single_file = os.path.join(BASE_PATH, 'single_text.txt')
doc_a_file = os.path.join(BASE_PATH, 'doc_a.txt')
doc_b_file = os.path.join(BASE_PATH, 'doc_b.txt')


print(f"æ­£åœ¨å°è¯•ä»åŸºç¡€è·¯å¾„åŠ è½½æ–‡ä»¶: {BASE_PATH}\n")

# --- è¿è¡Œä»»åŠ¡ 1 ---
calculate_single_text_mi(single_file)

# --- è¿è¡Œä»»åŠ¡ 2 ---
calculate_two_text_avg_mi(doc_a_file, doc_b_file)